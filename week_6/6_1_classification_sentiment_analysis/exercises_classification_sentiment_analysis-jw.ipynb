{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Exercise: Text Classification for Sentiment Analysis\n",
    "===\n",
    "\n",
    "![](http://choyeski.com/wp-content/uploads/2013/02/movie-review.jpg)\n",
    "\n",
    "\n",
    "Let's pick where we left off last session...\n",
    "\n",
    "Let's refine the best model so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Calcute the precision on the test data\n",
    "\n",
    "<br>\n",
    "<details><summary>\n",
    "Click here for the precision score...\n",
    "</summary>\n",
    "<br>\n",
    "85.65%\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.datasets.base.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import os\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "url = \"http://www.cs.cornell.edu/People/pabo/movie-review-data/\"\n",
    "filename = \"review_polarity.tar.gz\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url+filename, filename)\n",
    "    \n",
    "import tarfile\n",
    "\n",
    "path = \"./txt_sentoken/\"\n",
    "\n",
    "if not os.path.exists(path):\n",
    "    with tarfile.open(filename, \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "        \n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "# Load data\n",
    "sentiment = load_files(path, \n",
    "                       encoding='utf-8',\n",
    "                       random_state=42)\n",
    "sentiment.target_names\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Create train/test split with labels\n",
    "train_data, test_data, train_target, test_target = train_test_split(sentiment.data,\n",
    "                                                                    sentiment.target,\n",
    "                                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 35,350 words in the vocabulary.\n",
      "'bacon' appers 2,600 times.\n",
      "The accuracy on the test data is 82.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Transform train data from a list of strings into a matrix of frequency counts\n",
    "vectorizer_count = CountVectorizer()\n",
    "vectorized_count_train_data = vectorizer_count.fit_transform(train_data)\n",
    "\n",
    "print(\"There are {:,} words in the vocabulary.\".format(len(vectorizer_count.vocabulary_)))\n",
    "print(\"'{}' appers {:,} times.\".format('bacon', vectorizer_count.vocabulary_['bacon']))\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Create an instance of the Naive Bayes class \n",
    "clf = MultinomialNB()\n",
    "# Call fit method\n",
    "clf.fit(vectorized_count_train_data, train_target)\n",
    "\n",
    "accuracy = clf.score(vectorizer_count.transform(test_data), test_target)\n",
    "\n",
    "print(\"The accuracy on the test data is {:.2%}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = vectorizer_count.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 85.65%\n"
     ]
    }
   ],
   "source": [
    "# Calcute the precision on the test data\n",
    "# Click here for the precision score...\n",
    "\n",
    "# 85.65%\n",
    "\n",
    "precision_score?\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "precision_score(test_target, y_predict)\n",
    "print('Precision: {:.2%}'.format(precision_score(test_target, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Calcute the recall on the test data\n",
    "\n",
    "<br>\n",
    "<details><summary>\n",
    "Click here for the recall score...\n",
    "</summary>\n",
    "<br>\n",
    "78.38%\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 78.38%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# recall_score(test_target, y_predict)\n",
    "print('Recall: {:.2%}'.format(recall_score(test_target, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do you think one is so much higher than the other?\n",
    "\n",
    "What does that tell us about our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recall_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Precision is higher than recall because there are fewer false positives than false negatives.  The type II error is higher than type I.\n",
    "\n",
    "So we incorrectly predict bad reviews.  The model is pessimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "\n",
    "Calcute the $F_1$ score \n",
    "\n",
    "<br>\n",
    "<details><summary>\n",
    "Click here for the $F_1$ score...\n",
    "</summary>\n",
    "<br>\n",
    "$F_1$ = 80.86% - not the actual number\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `f1_score` not found.\n"
     ]
    }
   ],
   "source": [
    "f1_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 81.85%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# recall_score(test_target, y_predict)\n",
    "print('F1 score: {:.2%}'.format(f1_score(test_target, y_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the best evaluation metric for your model? Why pick that one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "the F1 score is usually the best metric.  Our precision is higher than recall, so there are more false negatives, and the model is pessimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "How could you improve your $F_1$ score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Get fewer false positives or false negatives, increase precision and recall, and get more true positives.\n",
    "\n",
    "We want fewer false negatives, change the parameters and threshold so that neutral reviews lean towards positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "Let's experiment with another sentiment tool: [TextBlob](https://textblob.readthedocs.org/en/dev/quickstart.html#sentiment-analysis)\n",
    "\n",
    "It comes pretrained..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=-0.35, subjectivity=0.7)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Roger Ebert hatin' http://www.rogerebert.com/reviews/north-1994\n",
    "\n",
    "testimonial = TextBlob(\"\"\"I have no idea why Rob Reiner, or anyone else, wanted to make this story into a movie, and close examination of the film itself is no help. \n",
    "\"North\" is one of the most unpleasant, contrived, artificial, cloying experiences I've had at the movies. \n",
    "To call it manipulative would be inaccurate; it has an ambition to manipulate, but fails\"\"\")\n",
    "\n",
    "testimonial.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.35"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testimonial.sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit TextBlob to the moview reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_polarity = [TextBlob(item).sentiment.polarity for item in str(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_pred = np.array([0 if item < 0 else 1 for item in X_test_polarity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d3699b141a53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print('Accuracy: {:.2%}'.format(accuracy_score(y_test, y_pred)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Precision: {:.2%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Recall: {:.2%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'F1_Score: {:.2%}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# print('Accuracy: {:.2%}'.format(accuracy_score(y_test, y_pred)))\n",
    "# print('Precision: {:.2%}'.format(precision_score(y_test, y_pred)))\n",
    "# print('Recall: {:.2%}'.format(recall_score(y_test, y_pred)))\n",
    "# print('F1_Score: {:.2%}'.format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also train TextBlob..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = [\n",
    "    ('I love this sandwich.', 'pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I feel very good about these beers.', 'pos'),\n",
    "    ('This is my best work.', 'pos'),\n",
    "    (\"What an awesome view\", 'pos'),\n",
    "    ('I do not like this restaurant', 'neg'),\n",
    "    ('I am tired of this stuff.', 'neg'),\n",
    "    (\"I can't deal with this\", 'neg'),\n",
    "    ('He is my sworn enemy!', 'neg'),\n",
    "    ('My boss is horrible.', 'neg')\n",
    "]\n",
    "test = [\n",
    "    ('The beer was good.', 'pos'),\n",
    "    ('I do not enjoy my job', 'neg'),\n",
    "    (\"I ain't feeling dandy today.\", 'neg'),\n",
    "    (\"I feel amazing!\", 'pos'),\n",
    "    ('Gary is a friend of mine.', 'pos'),\n",
    "    (\"I can't believe I'm doing this.\", 'neg')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl = NaiveBayesClassifier(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for review, label in test:\n",
    "    print(review)\n",
    "    print(\"  gold: \\t\", label)\n",
    "    print(\"  predicted: \\t\", cl.classify(review))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"{:.2}\".format(cl.accuracy(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl.classify(\"Their burgers are amazing\")  # \"pos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl.classify(\"I don't like their pizza.\") # neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cl.show_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train TextBlob on the moview reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "sentiment = load_files(path, \n",
    "                       encoding='utf-8', # You need to specific encoding for TextBlob; Optional for scikit-learn\n",
    "                       random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model would you deploy for a moview review site? Think about performance, speed, and ease of use.\n",
    "\n",
    "For example, [Rotten Tomatoes](https://www.rottentomatoes.com/m/battlefield_earth/) shows postive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Challenge exercises\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add features for negation\n",
    "\n",
    "Let's model negation as shown in the videos by adding `NOT_` to every word between negation and following punctuation:  \n",
    "    $e.g.$ `\"didn’t like this movie , but I\"`  \n",
    "    $\\rightarrow$ `\"didn’t NOT_like NOT_this NOT_movie but I\"`  \n",
    "\n",
    "See [Stack Overflow](http://stackoverflow.com/questions/23384351/how-to-add-tags-to-negated-words-in-strings-that-follow-not-no-and-never) for how an example of how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does adding negation improve your overall performance as measured in confusion matrix? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try other modes and hyperparameters. \n",
    "\n",
    "Can you find a model with higher F1 score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
